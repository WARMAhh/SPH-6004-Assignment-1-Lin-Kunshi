{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFPyQ2iEc1z"
      },
      "source": [
        "# SPH6004 Hands-On 3\n",
        "## Tree-based models and Support Vector Machines\n",
        "\n",
        "Contents:\n",
        "* New models\n",
        "  * Tree-based models\n",
        "    * Simple decision trees\n",
        "    * Gradient boosting trees\n",
        "  * Support vector machines (SVM)\n",
        "    * linear SVM\n",
        "    * SVM with the rbf kernel\n",
        "* `sklearn` techniques\n",
        "  * New model implementations in `sklearn`\n",
        "  * Parameter selection using `GridSearchCV`\n",
        "* Linearly separable and non-separable data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rZBXKn1_Ec12"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We will use the following packages\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# NumPy for math operations, and Pandas for processing tabular data.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Plotly plotting package\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_objects\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgo\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# We will use the following packages\n",
        "\n",
        "# NumPy for math operations, and Pandas for processing tabular data.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotly plotting package\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "# We use toy datasets in scikit-learn package\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Tools in sklearn to select best model\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
        "\n",
        "# Decision tree classifier in sklearn\n",
        "from sklearn.tree import DecisionTreeClassifier as DTC, plot_tree\n",
        "\n",
        "# We use f1 score to test model performance\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Import matplotlib.pyplot to visualize tree models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXeIyn7mEc13"
      },
      "source": [
        "Again we use the breast cancer data. Recall that from last hands-on, the forward feature selection method selected `mean concave points`, `radius error` and `worst concave points` as the top-3 most important features to predict whether cell samples are benign or malignant. (Note that the forward feature selection method has build-in randomness, so if you try it multiple times you may get different top-3 features.) To ease data and model visualization we only use these tree features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byuy8LsqEc13"
      },
      "outputs": [],
      "source": [
        "X_raw, y_df = load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "\n",
        "X_df = X_raw[['mean concave points', 'radius error', 'worst concave points']]\n",
        "Xy = pd.concat([X_df,y_df],axis=1)\n",
        "Xy['label'] = np.where(Xy['target'].to_numpy()==0,'benign','malignant')\n",
        "Xy.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWjQbZBTEc14"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter_3d(\n",
        "    Xy,\n",
        "    z='mean concave points',\n",
        "    y='radius error',\n",
        "    x='worst concave points',\n",
        "    color='label'\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker={'size':2.5}\n",
        ")\n",
        "\n",
        "surface_z = [[0.052,0.052],[0.052,0.052]]\n",
        "surface_y = [0.1,2.7]\n",
        "surface_x = [0,0.29]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Surface(\n",
        "        x=surface_x,\n",
        "        y=surface_y,\n",
        "        z=surface_z,\n",
        "        opacity=0.5,\n",
        "        showscale=False,\n",
        "        colorscale='greys',\n",
        "        name='separating hyperplane',\n",
        "        showlegend=True,\n",
        "        visible='legendonly'\n",
        "    )\n",
        ")\n",
        "\n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyBR60E_Ec15"
      },
      "outputs": [],
      "source": [
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_df,y_df,test_size=0.3,random_state=10,\n",
        "    stratify=y_df, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8ectaZPEc15"
      },
      "source": [
        "### Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5CW887sEc15"
      },
      "outputs": [],
      "source": [
        "# We first build a shallow decision tree.\n",
        "TreeModel = DTC(criterion='entropy',max_depth=1,random_state=15)\n",
        "TreeModel.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEcvjXbLEc16"
      },
      "outputs": [],
      "source": [
        "# Splitting rules can be visualized by using plot_tree in sklearn\n",
        "plt.figure(figsize=(14,10))\n",
        "plot_tree(\n",
        "    TreeModel,\n",
        "    filled=True,\n",
        "    feature_names=['mean concave points', 'radius error', 'worst concave points'],\n",
        "    class_names=['benign','malignant']\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBKttewn0b_c"
      },
      "outputs": [],
      "source": [
        "# The `max_depth` parameter is important for decision tree.\n",
        "# We use `GridSearchCV` to select the best `max_depth`.\n",
        "\n",
        "parameters = {'max_depth':np.arange(start=1,stop=10,step=1)}\n",
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPJE7wWe0c-C"
      },
      "outputs": [],
      "source": [
        "stratifiedCV = StratifiedKFold(n_splits=8)\n",
        "TreeModel = DTC(criterion='entropy')\n",
        "BestTree = GridSearchCV(\n",
        "    TreeModel,\n",
        "    param_grid=parameters,\n",
        "    scoring='f1',\n",
        "    cv=stratifiedCV\n",
        ")\n",
        "BestTree.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QOjwVHPEc16"
      },
      "outputs": [],
      "source": [
        "BestTree.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFwhxMAZEc16"
      },
      "outputs": [],
      "source": [
        "BestTree.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amd_8CQsEc16"
      },
      "outputs": [],
      "source": [
        "y_pred = BestTree.predict(X_test)\n",
        "print('F1 score on test set: {:.4f}'.format(f1_score(y_test,y_pred)))\n",
        "pd.crosstab(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfvhLHjsEc17"
      },
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_aR8OkUEc17"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier as XGBC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MVhJZArEc17"
      },
      "source": [
        "The `xgboost` is a powerful and popular gradient boosting tree model. However, it requires careful tuning of parameters. Some of important parameters:\n",
        "* `n_estimators`: number of gradient boosted trees\n",
        "* `max_depth`: maximum tree depth for each gradient boosted trees\n",
        "* `learning_rate`: the $\\eta$ in lecture notes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0A6JPF6bBzz"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'n_estimators':np.arange(start=2,stop=20,step=2),\n",
        "    'max_depth':np.arange(start=2,stop=6,step=1),\n",
        "    'learning_rate':np.arange(start=0.05,stop=0.4,step=0.05)\n",
        "}\n",
        "\n",
        "parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmRJ3W0SEc17"
      },
      "outputs": [],
      "source": [
        "stratifiedCV = StratifiedKFold(n_splits=8)\n",
        "# XGBC: XGBoost classifier\n",
        "XGBoostModel = XGBC()\n",
        "BestXGBoost = GridSearchCV(\n",
        "    XGBoostModel,\n",
        "    param_grid=parameters,\n",
        "    scoring='f1',\n",
        "    cv=stratifiedCV,\n",
        "    verbose=1,\n",
        "    n_jobs=-1 # use all cpu cores to speedup grid search\n",
        ")\n",
        "BestXGBoost.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LyVdFcgEc17"
      },
      "outputs": [],
      "source": [
        "BestXGBoost.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vIzBrsnEc17"
      },
      "outputs": [],
      "source": [
        "BestXGBoost.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pp_YvW7Ec17"
      },
      "outputs": [],
      "source": [
        "y_pred = BestXGBoost.predict(X_test)\n",
        "print('F1 score on test set: {:.4f}'.format(f1_score(y_test,y_pred)))\n",
        "pd.crosstab(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1aECGxIEc17"
      },
      "source": [
        "### Linear SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq-XyBQ8Ec17"
      },
      "source": [
        "Since our data seems to be linearly separable, we expect an SVM with the simple linear kernel can give us good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPzTjdiaEc17"
      },
      "outputs": [],
      "source": [
        "# Support Vector Classifier\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i02SS7tQEc17"
      },
      "outputs": [],
      "source": [
        "# 'C': strength of L2 regularization on linear SVM. Larger 'C' --> smaller regularization.\n",
        "parameters = {\n",
        "    'C':np.arange(start=1,stop=20,step=5)\n",
        "}\n",
        "stratifiedCV = StratifiedKFold(n_splits=8)\n",
        "SVCModel = SVC(kernel='linear')\n",
        "BestSVC = GridSearchCV(\n",
        "    SVCModel,\n",
        "    param_grid=parameters,\n",
        "    scoring='f1',\n",
        "    cv=stratifiedCV,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "BestSVC.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfbTqWLcEc18"
      },
      "outputs": [],
      "source": [
        "BestSVC.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROFtZfp1Ec18"
      },
      "outputs": [],
      "source": [
        "BestSVC.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLzNWKiREc18"
      },
      "outputs": [],
      "source": [
        "y_pred = BestSVC.predict(X_test)\n",
        "print('F1 score on test set: {:.4f}'.format(f1_score(y_test,y_pred)))\n",
        "pd.crosstab(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UdJLPfSEc18"
      },
      "source": [
        "## An example of linearly non-separable data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kH63zKtnEc18"
      },
      "outputs": [],
      "source": [
        "# Generate data\n",
        "r1,r2 = 0.1,2\n",
        "noise_level = .5\n",
        "sample_num = 1000\n",
        "theta = np.linspace(0,2*np.pi,num=sample_num)\n",
        "\n",
        "c1_x = r1*np.cos(theta)+np.random.normal(scale=noise_level,size=sample_num)\n",
        "c1_y = r1*np.sin(theta)+np.random.normal(scale=noise_level,size=sample_num)\n",
        "\n",
        "c2_x = r2*np.cos(theta)+np.random.normal(scale=noise_level,size=sample_num)\n",
        "c2_y = r2*np.sin(theta)+np.random.normal(scale=noise_level,size=sample_num)\n",
        "\n",
        "X = pd.DataFrame(\n",
        "    {\n",
        "        'x1':np.hstack([c1_x,c2_x]),\n",
        "        'x2':np.hstack([c1_y,c2_y])\n",
        "    }\n",
        ")\n",
        "y = pd.Series(\n",
        "    data=[0]*sample_num+[1]*sample_num,\n",
        "    name='label'\n",
        ")\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(\n",
        "    X,y,random_state=9,shuffle=True,test_size=0.3\n",
        ")\n",
        "\n",
        "Xy_test = pd.concat([X_test,y_test],axis=1)\n",
        "Xy_test['label'] = Xy_test['label'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vR8WbhE8Ec18"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    Xy_test,\n",
        "    x='x1',\n",
        "    y='x2',\n",
        "    color='label'\n",
        ")\n",
        "\n",
        "fig.update_yaxes(\n",
        "    scaleanchor='x',\n",
        "    scaleratio=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxRbmsiOEc18"
      },
      "source": [
        "### The failure of linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b1FIwa1Ec18"
      },
      "source": [
        "The above data is clearly not linearly separable, i.e. you cannot use a line to separate the two classes. Linear models we learned in this course, e.g. logistic regression and SVM with linear kernel, cannot handle such data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfF-kpQnEc18"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "\n",
        "linear_models = {\n",
        "    'LR':LR(),\n",
        "    'SVM_linear':SVC(kernel='linear')\n",
        "}\n",
        "\n",
        "stratifiedCV = StratifiedKFold(n_splits=8)\n",
        "\n",
        "\n",
        "params = {\n",
        "    'LR':{\n",
        "        'C':np.arange(0.5,20,step=1)\n",
        "    },\n",
        "    'SVM_linear':{\n",
        "        'C':np.arange(0.5,20,step=1)\n",
        "    }\n",
        "}\n",
        "\n",
        "records = {}\n",
        "\n",
        "for model in linear_models:\n",
        "    BestParams = GridSearchCV(\n",
        "        linear_models[model],\n",
        "        param_grid = params[model],\n",
        "        scoring='f1',\n",
        "        cv=stratifiedCV,\n",
        "        n_jobs=-1 # Use all cpu cores to speed-up CV search\n",
        "    )\n",
        "    BestParams.fit(X_train,y_train)\n",
        "    records[model] = BestParams\n",
        "    Xy_test[model] = BestParams.predict(X_test).astype(str)\n",
        "    print('For {} cross validation F1 score is {:.4f}'.format(model,BestParams.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nOqk9pREc18"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    Xy_test.melt(id_vars=['x1','x2'],value_vars=['label','LR','SVM_linear']),\n",
        "    x='x1',\n",
        "    y='x2',\n",
        "    facet_col='variable',\n",
        "    color='value'\n",
        ")\n",
        "fig.update_yaxes(\n",
        "    scaleanchor='x',\n",
        "    scaleratio=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2nE0BbBEc18"
      },
      "source": [
        "We can see that both linear methods can only try to separate the two classes by a line. Since data cannot be separated by a line, the prediction accuracy is poor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qeu_vwrMEc18"
      },
      "source": [
        "### Nonlinear methods\n",
        "\n",
        "Let us try to classify the two classes by non-linear methods, which include\n",
        "* Decision tree and its related models\n",
        "* SVM to rbf kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kblrBQE8Ec19"
      },
      "outputs": [],
      "source": [
        "nonlinear_models = {\n",
        "    'DecisionTree':DTC(criterion='entropy'),\n",
        "    'XGBoost':XGBC(),\n",
        "    'SVM_rbf':SVC(kernel='rbf')\n",
        "}\n",
        "\n",
        "stratifiedCV = StratifiedKFold(n_splits=8)\n",
        "\n",
        "\n",
        "params = {\n",
        "    'DecisionTree':{\n",
        "        'max_depth':np.arange(start=1,stop=10)\n",
        "    },\n",
        "    'XGBoost':{\n",
        "        'n_estimators':np.arange(start=2,stop=20,step=2),\n",
        "        'max_depth':np.arange(start=2,stop=6),\n",
        "        'learning_rate':np.arange(start=0.05,stop=0.4,step=0.05)\n",
        "    },\n",
        "    'SVM_rbf':{\n",
        "        'C':np.arange(0.5,5,step=0.5)\n",
        "    }\n",
        "}\n",
        "\n",
        "records = {}\n",
        "\n",
        "for model in nonlinear_models:\n",
        "    BestParams = GridSearchCV(\n",
        "        nonlinear_models[model],\n",
        "        param_grid = params[model],\n",
        "        scoring='f1',\n",
        "        cv=stratifiedCV,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    BestParams.fit(X_train,y_train)\n",
        "    Xy_test[model] = BestParams.predict(X_test).astype(str)\n",
        "    records[model] = BestParams\n",
        "    print('For {} cross validation F1 score is {:.4f}'.format(model,BestParams.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOcazXh6Ec19"
      },
      "outputs": [],
      "source": [
        "fig = px.scatter(\n",
        "    Xy_test.melt(id_vars=['x1','x2'],value_vars=['label','DecisionTree','XGBoost','SVM_rbf']),\n",
        "    x='x1',\n",
        "    y='x2',\n",
        "    facet_col='variable',\n",
        "    color='value'\n",
        ")\n",
        "fig.update_traces(\n",
        "    marker={'size':5}\n",
        ")\n",
        "fig.update_yaxes(\n",
        "    scaleanchor='x',\n",
        "    scaleratio=1\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5ec3c21330053bd888ae0423f793f928621505090c66dc4d3b71e8bb37792d66"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
